{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import _tree\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1910 examples in training, 216 examples for testing.\n",
      "1910 examples in new training after duplication.\n",
      "Use 11 thread(s) for training\n",
      "Use /var/folders/qq/8hq4nv8x5rz6hyggl1ylc_kh0000gn/T/tmpkd9o846c as temporary training directory\n",
      "Reading training dataset...\n",
      "Training tensor examples:\n",
      "Features: {'baseline_FHR_bpm': <tf.Tensor 'data:0' shape=(None,) dtype=float64>, 'accelerations': <tf.Tensor 'data_1:0' shape=(None,) dtype=float64>, 'fetal_movement': <tf.Tensor 'data_2:0' shape=(None,) dtype=float64>, 'uterine_contractions': <tf.Tensor 'data_3:0' shape=(None,) dtype=float64>, 'light_decelerations': <tf.Tensor 'data_4:0' shape=(None,) dtype=float64>, 'severe_decelerations': <tf.Tensor 'data_5:0' shape=(None,) dtype=float64>, 'prolonged_decelerations': <tf.Tensor 'data_6:0' shape=(None,) dtype=float64>, 'abnorm_ST_Var_Perc': <tf.Tensor 'data_7:0' shape=(None,) dtype=float64>, 'mean_ST_Var': <tf.Tensor 'data_8:0' shape=(None,) dtype=float64>, 'abnorm_LT_Var_Perc': <tf.Tensor 'data_9:0' shape=(None,) dtype=float64>, 'mean_LT_Var': <tf.Tensor 'data_10:0' shape=(None,) dtype=float64>, 'FHR_hist_width': <tf.Tensor 'data_11:0' shape=(None,) dtype=float64>, 'FHR_hist_min': <tf.Tensor 'data_12:0' shape=(None,) dtype=float64>, 'FHR_hist_max': <tf.Tensor 'data_13:0' shape=(None,) dtype=float64>, 'FHR_hist_num_peaks': <tf.Tensor 'data_14:0' shape=(None,) dtype=float64>, 'FHR_hist_num_zeroes': <tf.Tensor 'data_15:0' shape=(None,) dtype=float64>, 'FHR_hist_mode': <tf.Tensor 'data_16:0' shape=(None,) dtype=float64>, 'FHR_hist_mean': <tf.Tensor 'data_17:0' shape=(None,) dtype=float64>, 'FHR_hist_median': <tf.Tensor 'data_18:0' shape=(None,) dtype=float64>, 'FHR_hist_variance': <tf.Tensor 'data_19:0' shape=(None,) dtype=float64>, 'FHR_hist_tendency': <tf.Tensor 'data_20:0' shape=(None,) dtype=float64>}\n",
      "Label: Tensor(\"data_21:0\", shape=(None,), dtype=int64)\n",
      "Weights: None\n",
      "Normalized tensor features:\n",
      " {'baseline_FHR_bpm': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast:0' shape=(None,) dtype=float32>), 'accelerations': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_1:0' shape=(None,) dtype=float32>), 'fetal_movement': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_2:0' shape=(None,) dtype=float32>), 'uterine_contractions': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_3:0' shape=(None,) dtype=float32>), 'light_decelerations': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_4:0' shape=(None,) dtype=float32>), 'severe_decelerations': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_5:0' shape=(None,) dtype=float32>), 'prolonged_decelerations': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_6:0' shape=(None,) dtype=float32>), 'abnorm_ST_Var_Perc': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_7:0' shape=(None,) dtype=float32>), 'mean_ST_Var': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_8:0' shape=(None,) dtype=float32>), 'abnorm_LT_Var_Perc': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_9:0' shape=(None,) dtype=float32>), 'mean_LT_Var': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_10:0' shape=(None,) dtype=float32>), 'FHR_hist_width': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_11:0' shape=(None,) dtype=float32>), 'FHR_hist_min': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_12:0' shape=(None,) dtype=float32>), 'FHR_hist_max': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_13:0' shape=(None,) dtype=float32>), 'FHR_hist_num_peaks': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_14:0' shape=(None,) dtype=float32>), 'FHR_hist_num_zeroes': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_15:0' shape=(None,) dtype=float32>), 'FHR_hist_mode': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_16:0' shape=(None,) dtype=float32>), 'FHR_hist_mean': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_17:0' shape=(None,) dtype=float32>), 'FHR_hist_median': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_18:0' shape=(None,) dtype=float32>), 'FHR_hist_variance': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_19:0' shape=(None,) dtype=float32>), 'FHR_hist_tendency': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_20:0' shape=(None,) dtype=float32>)}\n",
      "Training dataset read in 0:00:00.163179. Found 1910 examples.\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 24-04-05 00:42:58.9038 EDT kernel.cc:771] Start Yggdrasil model training\n",
      "[INFO 24-04-05 00:42:58.9046 EDT kernel.cc:772] Collect training examples\n",
      "[INFO 24-04-05 00:42:58.9046 EDT kernel.cc:785] Dataspec guide:\n",
      "column_guides {\n",
      "  column_name_pattern: \"^__LABEL$\"\n",
      "  type: CATEGORICAL\n",
      "  categorial {\n",
      "    min_vocab_frequency: 0\n",
      "    max_vocab_count: -1\n",
      "  }\n",
      "}\n",
      "default_column_guide {\n",
      "  categorial {\n",
      "    max_vocab_count: 2000\n",
      "  }\n",
      "  discretized_numerical {\n",
      "    maximum_num_bins: 255\n",
      "  }\n",
      "}\n",
      "ignore_columns_without_guides: false\n",
      "detect_numerical_as_discretized_numerical: false\n",
      "\n",
      "[INFO 24-04-05 00:42:58.9048 EDT kernel.cc:391] Number of batches: 2\n",
      "[INFO 24-04-05 00:42:58.9048 EDT kernel.cc:392] Number of examples: 1910\n",
      "[INFO 24-04-05 00:42:58.9051 EDT kernel.cc:792] Training dataset:\n",
      "Number of records: 1910\n",
      "Number of columns: 22\n",
      "\n",
      "Number of columns by type:\n",
      "\tNUMERICAL: 21 (95.4545%)\n",
      "\tCATEGORICAL: 1 (4.54545%)\n",
      "\n",
      "Columns:\n",
      "\n",
      "NUMERICAL: 21 (95.4545%)\n",
      "\t0: \"FHR_hist_max\" NUMERICAL mean:164.1 min:122 max:238 sd:18.02\n",
      "\t1: \"FHR_hist_mean\" NUMERICAL mean:134.621 min:73 max:182 sd:15.6347\n",
      "\t2: \"FHR_hist_median\" NUMERICAL mean:138.097 min:77 max:186 sd:14.4614\n",
      "\t3: \"FHR_hist_min\" NUMERICAL mean:93.8476 min:50 max:159 sd:29.4998\n",
      "\t4: \"FHR_hist_mode\" NUMERICAL mean:137.404 min:60 max:187 sd:16.5363\n",
      "\t5: \"FHR_hist_num_peaks\" NUMERICAL mean:4.08115 min:0 max:18 sd:2.9529\n",
      "\t6: \"FHR_hist_num_zeroes\" NUMERICAL mean:0.320942 min:0 max:10 sd:0.690346\n",
      "\t7: \"FHR_hist_tendency\" NUMERICAL mean:0.317801 min:-1 max:1 sd:0.616567\n",
      "\t8: \"FHR_hist_variance\" NUMERICAL mean:18.7948 min:0 max:269 sd:29.28\n",
      "\t9: \"FHR_hist_width\" NUMERICAL mean:70.2524 min:3 max:180 sd:38.9491\n",
      "\t11: \"abnorm_LT_Var_Perc\" NUMERICAL mean:9.86754 min:0 max:91 sd:18.3886\n",
      "\t12: \"abnorm_ST_Var_Perc\" NUMERICAL mean:47.1136 min:12 max:87 sd:17.119\n",
      "\t13: \"accelerations\" NUMERICAL mean:0.00318168 min:0 max:0.019 sd:0.00387757\n",
      "\t14: \"baseline_FHR_bpm\" NUMERICAL mean:133.323 min:106 max:160 sd:9.8515\n",
      "\t15: \"fetal_movement\" NUMERICAL mean:0.00900681 min:0 max:0.481 sd:0.045278\n",
      "\t16: \"light_decelerations\" NUMERICAL mean:0.00188796 min:0 max:0.015 sd:0.00298197\n",
      "\t17: \"mean_LT_Var\" NUMERICAL mean:8.08974 min:0 max:41.8 sd:5.50575\n",
      "\t18: \"mean_ST_Var\" NUMERICAL mean:1.32675 min:0.2 max:7 sd:0.882317\n",
      "\t19: \"prolonged_decelerations\" NUMERICAL mean:0.00015288 min:0 max:0.005 sd:0.000570367\n",
      "\t20: \"severe_decelerations\" NUMERICAL mean:3.14136e-06 min:0 max:0.001 sd:5.59597e-05\n",
      "\t21: \"uterine_contractions\" NUMERICAL mean:0.00436754 min:0 max:0.015 sd:0.00295941\n",
      "\n",
      "CATEGORICAL: 1 (4.54545%)\n",
      "\t10: \"__LABEL\" CATEGORICAL integerized vocab-size:5 no-ood-item\n",
      "\n",
      "Terminology:\n",
      "\tnas: Number of non-available (i.e. missing) values.\n",
      "\tood: Out of dictionary.\n",
      "\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n",
      "\ttokenized: The attribute value is obtained through tokenization.\n",
      "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
      "\tvocab-size: Number of unique values.\n",
      "\n",
      "[INFO 24-04-05 00:42:58.9051 EDT kernel.cc:808] Configure learner\n",
      "[INFO 24-04-05 00:42:58.9053 EDT kernel.cc:822] Training config:\n",
      "learner: \"RANDOM_FOREST\"\n",
      "features: \"^FHR_hist_max$\"\n",
      "features: \"^FHR_hist_mean$\"\n",
      "features: \"^FHR_hist_median$\"\n",
      "features: \"^FHR_hist_min$\"\n",
      "features: \"^FHR_hist_mode$\"\n",
      "features: \"^FHR_hist_num_peaks$\"\n",
      "features: \"^FHR_hist_num_zeroes$\"\n",
      "features: \"^FHR_hist_tendency$\"\n",
      "features: \"^FHR_hist_variance$\"\n",
      "features: \"^FHR_hist_width$\"\n",
      "features: \"^abnorm_LT_Var_Perc$\"\n",
      "features: \"^abnorm_ST_Var_Perc$\"\n",
      "features: \"^accelerations$\"\n",
      "features: \"^baseline_FHR_bpm$\"\n",
      "features: \"^fetal_movement$\"\n",
      "features: \"^light_decelerations$\"\n",
      "features: \"^mean_LT_Var$\"\n",
      "features: \"^mean_ST_Var$\"\n",
      "features: \"^prolonged_decelerations$\"\n",
      "features: \"^severe_decelerations$\"\n",
      "features: \"^uterine_contractions$\"\n",
      "label: \"^__LABEL$\"\n",
      "task: CLASSIFICATION\n",
      "random_seed: 123456\n",
      "metadata {\n",
      "  framework: \"TF Keras\"\n",
      "}\n",
      "pure_serving_model: false\n",
      "[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n",
      "  num_trees: 300\n",
      "  decision_tree {\n",
      "    max_depth: 8\n",
      "    min_examples: 5\n",
      "    in_split_min_examples_check: true\n",
      "    keep_non_leaf_label_distribution: true\n",
      "    num_candidate_attributes: 21\n",
      "    missing_value_policy: GLOBAL_IMPUTATION\n",
      "    allow_na_conditions: false\n",
      "    categorical_set_greedy_forward {\n",
      "      sampling: 0.1\n",
      "      max_num_items: -1\n",
      "      min_item_frequency: 1\n",
      "    }\n",
      "    growing_strategy_best_first_global {\n",
      "    }\n",
      "    categorical {\n",
      "      cart {\n",
      "      }\n",
      "    }\n",
      "    axis_aligned_split {\n",
      "    }\n",
      "    internal {\n",
      "      sorting_strategy: PRESORTED\n",
      "    }\n",
      "    uplift {\n",
      "      min_examples_in_treatment: 5\n",
      "      split_score: KULLBACK_LEIBLER\n",
      "    }\n",
      "  }\n",
      "  winner_take_all_inference: true\n",
      "  compute_oob_performances: true\n",
      "  compute_oob_variable_importances: false\n",
      "  num_oob_variable_importances_permutations: 1\n",
      "  bootstrap_training_dataset: true\n",
      "  bootstrap_size_ratio: 1\n",
      "  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n",
      "  sampling_with_replacement: true\n",
      "}\n",
      "\n",
      "[INFO 24-04-05 00:42:58.9059 EDT kernel.cc:825] Deployment config:\n",
      "cache_path: \"/var/folders/qq/8hq4nv8x5rz6hyggl1ylc_kh0000gn/T/tmpkd9o846c/working_cache\"\n",
      "num_threads: 11\n",
      "try_resume_training: true\n",
      "\n",
      "[INFO 24-04-05 00:42:58.9060 EDT kernel.cc:887] Train model\n",
      "[INFO 24-04-05 00:42:58.9061 EDT random_forest.cc:416] Training random forest on 1910 example(s) and 21 feature(s).\n",
      "[INFO 24-04-05 00:42:58.9128 EDT random_forest.cc:802] Training of tree  1/300 (tree index:10) done accuracy:0.906667 logloss:3.36407\n",
      "[INFO 24-04-05 00:42:58.9199 EDT random_forest.cc:802] Training of tree  11/300 (tree index:13) done accuracy:0.930907 logloss:1.20367\n",
      "[INFO 24-04-05 00:42:58.9226 EDT random_forest.cc:802] Training of tree  21/300 (tree index:15) done accuracy:0.934555 logloss:0.766714\n",
      "[INFO 24-04-05 00:42:58.9259 EDT random_forest.cc:802] Training of tree  31/300 (tree index:30) done accuracy:0.935079 logloss:0.586704\n",
      "[INFO 24-04-05 00:42:58.9303 EDT random_forest.cc:802] Training of tree  41/300 (tree index:42) done accuracy:0.937173 logloss:0.429615\n",
      "[INFO 24-04-05 00:42:58.9335 EDT random_forest.cc:802] Training of tree  51/300 (tree index:53) done accuracy:0.937173 logloss:0.361163\n",
      "[INFO 24-04-05 00:42:58.9375 EDT random_forest.cc:802] Training of tree  61/300 (tree index:60) done accuracy:0.936649 logloss:0.345915\n",
      "[INFO 24-04-05 00:42:58.9430 EDT random_forest.cc:802] Training of tree  71/300 (tree index:76) done accuracy:0.940314 logloss:0.309134\n",
      "[INFO 24-04-05 00:42:58.9457 EDT random_forest.cc:802] Training of tree  81/300 (tree index:81) done accuracy:0.939791 logloss:0.273842\n",
      "[INFO 24-04-05 00:42:58.9505 EDT random_forest.cc:802] Training of tree  91/300 (tree index:88) done accuracy:0.939791 logloss:0.25703\n",
      "[INFO 24-04-05 00:42:58.9543 EDT random_forest.cc:802] Training of tree  101/300 (tree index:106) done accuracy:0.939791 logloss:0.23948\n",
      "[INFO 24-04-05 00:42:58.9580 EDT random_forest.cc:802] Training of tree  111/300 (tree index:107) done accuracy:0.940314 logloss:0.239386\n",
      "[INFO 24-04-05 00:42:58.9621 EDT random_forest.cc:802] Training of tree  121/300 (tree index:123) done accuracy:0.940314 logloss:0.239432\n",
      "[INFO 24-04-05 00:42:58.9672 EDT random_forest.cc:802] Training of tree  132/300 (tree index:132) done accuracy:0.940838 logloss:0.23931\n",
      "[INFO 24-04-05 00:42:58.9717 EDT random_forest.cc:802] Training of tree  142/300 (tree index:140) done accuracy:0.940838 logloss:0.239118\n",
      "[INFO 24-04-05 00:42:58.9755 EDT random_forest.cc:802] Training of tree  152/300 (tree index:149) done accuracy:0.940838 logloss:0.239147\n",
      "[INFO 24-04-05 00:42:58.9797 EDT random_forest.cc:802] Training of tree  162/300 (tree index:160) done accuracy:0.940838 logloss:0.239533\n",
      "[INFO 24-04-05 00:42:58.9845 EDT random_forest.cc:802] Training of tree  172/300 (tree index:175) done accuracy:0.939267 logloss:0.240555\n",
      "[INFO 24-04-05 00:42:58.9886 EDT random_forest.cc:802] Training of tree  182/300 (tree index:185) done accuracy:0.939267 logloss:0.240605\n",
      "[INFO 24-04-05 00:42:58.9921 EDT random_forest.cc:802] Training of tree  192/300 (tree index:193) done accuracy:0.939791 logloss:0.240659\n",
      "[INFO 24-04-05 00:42:58.9965 EDT random_forest.cc:802] Training of tree  202/300 (tree index:197) done accuracy:0.938743 logloss:0.240476\n",
      "[INFO 24-04-05 00:42:58.9986 EDT random_forest.cc:802] Training of tree  212/300 (tree index:198) done accuracy:0.939267 logloss:0.240389\n",
      "[INFO 24-04-05 00:42:59.0036 EDT random_forest.cc:802] Training of tree  223/300 (tree index:226) done accuracy:0.939267 logloss:0.240743\n",
      "[INFO 24-04-05 00:42:59.0085 EDT random_forest.cc:802] Training of tree  233/300 (tree index:230) done accuracy:0.940314 logloss:0.2406\n",
      "[INFO 24-04-05 00:42:59.0118 EDT random_forest.cc:802] Training of tree  243/300 (tree index:246) done accuracy:0.938743 logloss:0.240386\n",
      "[INFO 24-04-05 00:42:59.0157 EDT random_forest.cc:802] Training of tree  253/300 (tree index:255) done accuracy:0.939791 logloss:0.239829\n",
      "[INFO 24-04-05 00:42:59.0193 EDT random_forest.cc:802] Training of tree  263/300 (tree index:263) done accuracy:0.940314 logloss:0.222922\n",
      "[INFO 24-04-05 00:42:59.0229 EDT random_forest.cc:802] Training of tree  273/300 (tree index:270) done accuracy:0.939791 logloss:0.223307\n",
      "[INFO 24-04-05 00:42:59.0264 EDT random_forest.cc:802] Training of tree  283/300 (tree index:286) done accuracy:0.939791 logloss:0.223268\n",
      "[INFO 24-04-05 00:42:59.0307 EDT random_forest.cc:802] Training of tree  293/300 (tree index:283) done accuracy:0.940838 logloss:0.223393\n",
      "[INFO 24-04-05 00:42:59.0322 EDT random_forest.cc:802] Training of tree  300/300 (tree index:299) done accuracy:0.939791 logloss:0.222984\n",
      "[INFO 24-04-05 00:42:59.0323 EDT random_forest.cc:882] Final OOB metrics: accuracy:0.939791 logloss:0.222984\n",
      "[INFO 24-04-05 00:42:59.0340 EDT kernel.cc:919] Export model in log directory: /var/folders/qq/8hq4nv8x5rz6hyggl1ylc_kh0000gn/T/tmpkd9o846c with prefix ff4d3ef6d3d74b56\n",
      "[INFO 24-04-05 00:42:59.0396 EDT kernel.cc:937] Save model in resources\n",
      "[INFO 24-04-05 00:42:59.0405 EDT abstract_model.cc:881] Model self evaluation:\n",
      "Number of predictions (without weights): 1910\n",
      "Number of predictions (with weights): 1910\n",
      "Task: CLASSIFICATION\n",
      "Label: __LABEL\n",
      "\n",
      "Accuracy: 0.939791  CI95[W][0.930062 0.948499]\n",
      "LogLoss: : 0.222984\n",
      "ErrorRate: : 0.0602095\n",
      "\n",
      "Default Accuracy: : 0.773822\n",
      "Default LogLoss: : 0.682248\n",
      "Default ErrorRate: : 0.226178\n",
      "\n",
      "Confusion Table:\n",
      "truth\\prediction\n",
      "   1     2    3    4\n",
      "1  0     0    0    0\n",
      "2  0  1447   22    9\n",
      "3  0    70  205    2\n",
      "4  0     9    3  143\n",
      "Total: 1910\n",
      "\n",
      "\n",
      "[INFO 24-04-05 00:42:59.0476 EDT kernel.cc:1233] Loading model from path /var/folders/qq/8hq4nv8x5rz6hyggl1ylc_kh0000gn/T/tmpkd9o846c/model/ with prefix ff4d3ef6d3d74b56\n",
      "[INFO 24-04-05 00:42:59.0667 EDT decision_forest.cc:734] Model loaded with 300 root(s), 18300 node(s), and 20 input feature(s).\n",
      "[INFO 24-04-05 00:42:59.0667 EDT abstract_model.cc:1344] Engine \"RandomForestGeneric\" built\n",
      "[INFO 24-04-05 00:42:59.0667 EDT kernel.cc:1061] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 0:00:00.173624\n",
      "Compiling model...\n",
      "Model compiled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x34be6abd0>"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_decision_forests as tfdf\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to split dataset into training and testing datasets\n",
    "def split_dataset(dataset, test_ratio=0.1):\n",
    "    \"\"\"Splits a pandas DataFrame into training and testing datasets.\"\"\"\n",
    "    test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "    return dataset[~test_indices], dataset[test_indices]\n",
    "\n",
    "# Load the dataset\n",
    "data = Path('../Resources/clean_fetal_health.csv')\n",
    "dataset_df = pd.read_csv(data)\n",
    "\n",
    "# Split the dataset into training and testing datasets\n",
    "train_ds_pd, test_ds_pd = split_dataset(dataset_df, test_ratio = 0.1)\n",
    "print(\"{} examples in training, {} examples for testing.\".format(len(train_ds_pd), len(test_ds_pd)))\n",
    "\n",
    "# Function to duplicate the training dataset\n",
    "def duplicate_dataset(dataset, num_duplicates=2):\n",
    "    \"\"\"Duplicates the training dataset.\"\"\"\n",
    "    duplicated_datasets = [dataset] * num_duplicates\n",
    "    return pd.concat(duplicated_datasets)\n",
    "\n",
    "# Increase the size of the training dataset by duplicating it\n",
    "num_duplicates = 1  # Specify how many times you want to duplicate the dataset\n",
    "train_ds_pd = duplicate_dataset(train_ds_pd, num_duplicates)\n",
    "\n",
    "# Print the new training size\n",
    "print(\"{} examples in new training after duplication.\".format(len(train_ds_pd)))\n",
    "\n",
    "# Convert the training and testing datasets to TensorFlow datasets\n",
    "label = \"fetal_health\"\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label)\n",
    "\n",
    "# Specify the model\n",
    "num_candidate_attributes = min(50, len(train_ds_pd.columns) - 1)\n",
    "\n",
    "model_1 = tfdf.keras.RandomForestModel(\n",
    "    num_candidate_attributes=num_candidate_attributes,\n",
    "    verbose=2, \n",
    "    growing_strategy=\"BEST_FIRST_GLOBAL\",\n",
    "    max_depth=8,\n",
    "    min_examples=5)\n",
    "\n",
    "# Train the model\n",
    "model_1.fit(train_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0000e+00 - accuracy: 0.9537\n",
      "\n",
      "loss: 0.0000\n",
      "accuracy: 0.9537\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0000e+00 - accuracy: 0.9537\n",
      "\n",
      "loss: 0.0000\n",
      "accuracy: 0.9537\n",
      "7/7 [==============================] - 0s 765us/step\n",
      "Prediction probabilities:\n",
      "[[0.         0.11666662 0.10666663 0.77666605]\n",
      " [0.         0.02       0.03333334 0.9466659 ]\n",
      " [0.         0.00333333 0.         0.99666584]\n",
      " [0.         0.9466659  0.02       0.03333334]\n",
      " [0.         0.         0.12666662 0.8733326 ]\n",
      " [0.         0.8499993  0.14999993 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.81999934 0.1799999  0.        ]\n",
      " [0.         0.9933325  0.00666667 0.        ]\n",
      " [0.         0.816666   0.18333323 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.00333333 0.         0.99666584]\n",
      " [0.         0.77666605 0.2233332  0.        ]\n",
      " [0.         0.00666667 0.9933325  0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.826666   0.17333324 0.        ]\n",
      " [0.         0.6833328  0.31666645 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.64666617 0.3533331  0.        ]\n",
      " [0.         0.5533329  0.44666633 0.        ]\n",
      " [0.         0.98666584 0.01333333 0.        ]\n",
      " [0.         0.9899992  0.01       0.        ]\n",
      " [0.         0.21666653 0.7599994  0.02333334]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.         0.99999917 0.        ]\n",
      " [0.         0.00333333 0.99666584 0.        ]\n",
      " [0.         0.         0.         0.99999917]\n",
      " [0.         0.         0.         0.99999917]\n",
      " [0.         0.06666667 0.00333333 0.92999923]\n",
      " [0.         0.00333333 0.99666584 0.        ]\n",
      " [0.         0.15666659 0.8399993  0.00333333]\n",
      " [0.         0.00333333 0.99666584 0.        ]\n",
      " [0.         0.04666667 0.95333254 0.        ]\n",
      " [0.         0.82333267 0.17666657 0.        ]\n",
      " [0.         0.3666664  0.63333285 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.2466665  0.75333273 0.        ]\n",
      " [0.         0.89999926 0.09999997 0.        ]\n",
      " [0.         0.03       0.         0.9699992 ]\n",
      " [0.         0.         0.99999917 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.05       0.9499992  0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.98666584 0.01333333 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.9833325  0.01666667 0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.9366659  0.06333334 0.        ]\n",
      " [0.         0.9833325  0.01666667 0.        ]\n",
      " [0.         0.9366659  0.06333334 0.        ]\n",
      " [0.         0.98666584 0.01333333 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99666584 0.         0.00333333]\n",
      " [0.         0.01333333 0.03       0.9566659 ]\n",
      " [0.         0.9933325  0.00666667 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.5166663  0.32666644 0.15666659]\n",
      " [0.         0.9899992  0.01       0.        ]\n",
      " [0.         0.01333333 0.03666667 0.9499992 ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.98666584 0.01333333 0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.00666667 0.9933325  0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.9933325  0.00666667 0.        ]\n",
      " [0.         0.02666667 0.94333255 0.03      ]\n",
      " [0.         0.5499996  0.44999966 0.        ]\n",
      " [0.         0.05       0.9499992  0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.01666667 0.9833325  0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.9566659  0.04333334 0.        ]\n",
      " [0.         0.9899992  0.01       0.        ]\n",
      " [0.         0.9833325  0.01666667 0.        ]\n",
      " [0.         0.9933325  0.00666667 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.96666586 0.03333334 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.97666585 0.02333334 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.98666584 0.01333333 0.        ]\n",
      " [0.         0.9899992  0.01       0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.97666585 0.00666667 0.01666667]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.8033327  0.09999997 0.09666664]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.7699994  0.09999997 0.12999995]\n",
      " [0.         0.03       0.06000001 0.90999925]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.69999945 0.2999998  0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.9733325  0.02666667 0.        ]\n",
      " [0.         0.9799992  0.02       0.        ]\n",
      " [0.         0.9499992  0.05       0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.9899992  0.01       0.        ]\n",
      " [0.         0.54666626 0.25333318 0.19999988]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.9933325  0.00666667 0.        ]\n",
      " [0.         0.94333255 0.05666667 0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.9133326  0.08666665 0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.99666584 0.         0.00333333]\n",
      " [0.         0.9599992  0.01666667 0.02333334]\n",
      " [0.         0.98666584 0.00666667 0.00666667]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.92999923 0.01       0.06000001]\n",
      " [0.         0.9899992  0.01       0.        ]\n",
      " [0.         0.9933325  0.00333333 0.00333333]\n",
      " [0.         0.9133326  0.08666665 0.        ]\n",
      " [0.         0.99666584 0.00333333 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.2466665  0.01333333 0.7399994 ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.02666667 0.         0.9733325 ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99666584 0.         0.00333333]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.9833325  0.01666667 0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.9366659  0.05666667 0.00666667]\n",
      " [0.         0.99666584 0.         0.00333333]\n",
      " [0.         0.03       0.         0.9699992 ]\n",
      " [0.         0.3099998  0.01333333 0.67666614]\n",
      " [0.         0.         0.         0.99999917]\n",
      " [0.         0.         0.         0.99999917]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.00333333 0.00666667 0.9899992 ]\n",
      " [0.         0.99999917 0.         0.        ]\n",
      " [0.         0.9566659  0.04333334 0.        ]\n",
      " [0.         0.1466666  0.         0.85333264]\n",
      " [0.         0.22999986 0.         0.7699994 ]\n",
      " [0.         0.8899993  0.10999996 0.        ]\n",
      " [0.         0.6499995  0.3433331  0.00666667]\n",
      " [0.         0.8033327  0.19666655 0.        ]\n",
      " [0.         0.11333329 0.02       0.86666596]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 00:42:59.438426: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(metrics=[\"accuracy\"])\n",
    "evaluation = model_1.evaluate(test_ds, return_dict=True)\n",
    "print()\n",
    "\n",
    "for name, value in evaluation.items():\n",
    "  print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "# Compile the model with metrics\n",
    "model_1.compile(metrics=[\"accuracy\"])\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "evaluation = model_1.evaluate(test_ds, return_dict=True)\n",
    "print()\n",
    "\n",
    "# Print evaluation metrics\n",
    "for name, value in evaluation.items():\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "# Iterate through the test dataset and print prediction probabilities\n",
    "for features, label in test_ds:\n",
    "    predictions = model_1.predict(features)  # Get predictions for the current batch\n",
    "    print(\"Prediction probabilities:\")\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_saved_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing table_initializer.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 24-04-05 00:42:59.6572 EDT kernel.cc:1233] Loading model from path ./my_saved_model/assets/ with prefix ff4d3ef6d3d74b56\n",
      "[INFO 24-04-05 00:42:59.6805 EDT decision_forest.cc:734] Model loaded with 300 root(s), 18300 node(s), and 20 input feature(s).\n",
      "[INFO 24-04-05 00:42:59.6805 EDT kernel.cc:1061] Use fast generic engine\n",
      "WARNING:tensorflow:Issue encountered when serializing table_initializer.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight StatefulPartitionedCall/random_forest_model_36/StatefulPartitionedCall/RaggedConstant/Const with shape (1,) and dtype int64 was auto converted to the type int32\n",
      "weight StatefulPartitionedCall/random_forest_model_36/StatefulPartitionedCall/RaggedConstant/Const_1 with shape (1,) and dtype int64 was auto converted to the type int32\n"
     ]
    }
   ],
   "source": [
    "# Prepare and load the model with TensorFlow\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "# Save the model in the SavedModel format\n",
    "tf.saved_model.save(model_1, \"./my_saved_model\")\n",
    "\n",
    "# Convert the SavedModel to TensorFlow.js and save as a zip file\n",
    "tfjs.converters.tf_saved_model_conversion_v2.convert_tf_saved_model(\"./my_saved_model\", \"./tfjs_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
